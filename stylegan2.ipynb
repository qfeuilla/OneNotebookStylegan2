{
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "fe55b9f24d8452108871416900996aa93b6395251a5e4265f5d848b4a5f6394b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math\r\n",
    "from typing import Tuple, Optional, List, Iterator\r\n",
    "from pathlib import Path\r\n",
    "from IPython.display import clear_output\r\n",
    "\r\n",
    "import torchvision\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.utils.data as data\r\n",
    "from torch import nn\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Weight normalizer that help the optimizer with very large and small weight \r\n",
    "class EqualizedWeight(nn.Module):\r\n",
    "    def __init__(self, shape : List[int]):\r\n",
    "        super().__init__()\r\n",
    "        self.c = 1 / math.sqrt(np.prod(shape[1:]))\r\n",
    "        self.weight = nn.Parameter(torch.randn(shape))\r\n",
    "\r\n",
    "    def forward(self):\r\n",
    "        return self.weight * self.c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Linear module with equalized weight\r\n",
    "class EqualizedLinear(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, in_features : int, out_features : int, bias : float = 0.):\r\n",
    "        super().__init__()\r\n",
    "        self.weight = EqualizedWeight([out_features, in_features])\r\n",
    "        self.bias = nn.Parameter(torch.ones(out_features) * bias)\r\n",
    "    \r\n",
    "    def forward(self, x : torch.Tensor):\r\n",
    "        return F.linear(x, self.weight(), bias=self.bias)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Conv2d module with equalized weight\r\n",
    "class EqualizedConv2d(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, in_features : int, out_features : int, kernel_size : int, padding : int = 0):\r\n",
    "        super().__init__()\r\n",
    "        self.padding = padding\r\n",
    "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\r\n",
    "        self.bias = nn.Parameter(torch.ones(out_features))\r\n",
    "    \r\n",
    "    def forward(self, x : torch.Tensor):\r\n",
    "        return F.conv2d(x, self.weight(), bias = self.bias, padding = self.padding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Blur the image module (help to clear patches and irregularities)\r\n",
    "class Smooth(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # Blur kernel\r\n",
    "        kernel = [\r\n",
    "            [1, 2, 1],\r\n",
    "            [2, 4, 2],\r\n",
    "            [1, 2, 1]\r\n",
    "        ]\r\n",
    "\r\n",
    "        kernel = torch.tensor([[kernel]], dtype = torch.float)\r\n",
    "        # normalize the kernel\r\n",
    "        kernel /= kernel.sum()\r\n",
    "\r\n",
    "        # this layer isn't trained\r\n",
    "        self.kernel = nn.Parameter(kernel, requires_grad=False)\r\n",
    "        # used to smooth the edge by clonning each edge by 1\r\n",
    "        self.pad = nn.ReplicationPad2d(1)\r\n",
    "\r\n",
    "    def forward(self, x : torch.Tensor):\r\n",
    "        batch, chan, height, width = x.shape\r\n",
    "\r\n",
    "        # Unfolding the channels has batch * chan images for the conv2d operation\r\n",
    "        # Because we dont want the images to blur between the channels\r\n",
    "        x = x.view(-1, 1, height, width)\r\n",
    "        x = self.pad(x)\r\n",
    "        x = F.conv2d(x, self.kernel)\r\n",
    "\r\n",
    "        return x.view(batch, chan, height, width)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Smooth each feature channels and scale down by 2 using bilinear interpolation\r\n",
    "class DownSample(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, use_smooth=True):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.use_smooth = use_smooth\r\n",
    "        if use_smooth:\r\n",
    "            self.smooth = Smooth()\r\n",
    "    \r\n",
    "    def forward(self, x : torch.Tensor):\r\n",
    "        if self.use_smooth:\r\n",
    "            x = self.smooth(x)\r\n",
    "        return F.interpolate(x, (x.shape[2] // 2, x.shape[3] // 2), mode=\"bilinear\", align_corners=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Smooth each feature channels and scale up by 2 using bilinear torch upsampling layer\r\n",
    "class UpSample(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, use_smooth=True):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.use_smooth = use_smooth\r\n",
    "        if use_smooth:\r\n",
    "            self.smooth = Smooth()\r\n",
    "        \r\n",
    "        self.up_sample = nn.Upsample(scale_factor = 2, mode=\"bilinear\", align_corners=False)\r\n",
    "    \r\n",
    "    def forward(self, x : torch.Tensor):\r\n",
    "        x = self.up_sample(x)\r\n",
    "\r\n",
    "        if self.use_smooth:\r\n",
    "            x = self.smooth(x)\r\n",
    "        \r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Mini-batch Standard Deviation\r\n",
    "class MiniBatchStdDev(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Mini-batch standard deviation calculates the standard deviation\r\n",
    "    across a mini-batch (or a subgroups within the mini-batch)\r\n",
    "    for each feature in the feature map. Then it takes the mean of all\r\n",
    "    the standard deviations and appends it to the feature map as one extra feature.\r\n",
    "    It also helps with the model collapse problem\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, group_size=4, num_new_features=1):\r\n",
    "        super().__init__()\r\n",
    "        self.group_size = group_size\r\n",
    "        self.num_new_features = num_new_features    \r\n",
    "\r\n",
    "    def forward(self, x: torch.Tensor):\r\n",
    "        b, c, h, w = x.shape\r\n",
    "        g = torch.min(torch.as_tensor(self.group_size), torch.as_tensor(b)) if self.group_size is not None else b\r\n",
    "        f = self.num_new_features\r\n",
    "        c = c // f\r\n",
    "\r\n",
    "        # Split minibatch N into n groups of size G, and channels C into F groups of size c.\r\n",
    "        y = x.reshape(g, -1, f, c, h, w)\r\n",
    "        # Subtract mean over group.\r\n",
    "        y = y - y.mean(dim=0)\r\n",
    "        # Calculate variance over group.\r\n",
    "        y = y.square().mean(dim=0)\r\n",
    "        # Calculate stddev over group.\r\n",
    "        y = (y + 1e-8).sqrt()\r\n",
    "        # Take average over channels and pixels.\r\n",
    "        y = y.mean(dim=[2,3,4])\r\n",
    "        # Add missing dimensions.\r\n",
    "        y = y.reshape(-1, f, 1, 1)\r\n",
    "        # Replicate over group and pixels.\r\n",
    "        y = y.repeat(g, 1, h, w)\r\n",
    "        # Append to input as new channels.\r\n",
    "        x = torch.cat([x, y], dim=1)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"mapping_network.svg\">\r\n",
    "\r\n",
    "First the mapping network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# map the latent vector to an intermediate latent space\r\n",
    "class MappingNetwork(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, features : int, n_layers : int):\r\n",
    "        super().__init__()\r\n",
    "        layers = []\r\n",
    "        for i in range(n_layers):\r\n",
    "            layers.append(EqualizedLinear(features, features))\r\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\r\n",
    "        self.net = nn.Sequential(*layers)\r\n",
    "    \r\n",
    "    def forward(self, z : torch.Tensor):\r\n",
    "        z = F.normalize(z, dim = 1)\r\n",
    "        return self.net(z)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# weigh modulation and demodulation operation (scale by the style vector and demodulate by normalize the result)\r\n",
    "class Conv2dWeightModulate(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, in_features: int, out_features : int, kernel_size : int, demodulate : bool = True, eps : float = 1e-8):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.out_features = out_features\r\n",
    "        self.demodulate = demodulate\r\n",
    "        self.padding = (kernel_size - 1) // 2\r\n",
    "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\r\n",
    "        self.eps = eps\r\n",
    "\r\n",
    "    def forward(self, x : torch.Tensor, s : torch.Tensor):\r\n",
    "        '''\r\n",
    "        x of shape [batch_size, in_features, out_features, height, width]\r\n",
    "        s of shape [batch_size, in_features]\r\n",
    "        '''\r\n",
    "        b, _, h, w = x.shape\r\n",
    "        # reshaping the style tensor\r\n",
    "        s = s[:, None, :, None, None]\r\n",
    "        weights = self.weight()[None, :, :, :, :]\r\n",
    "\r\n",
    "        # weight modulation by style tensor\r\n",
    "        weights = weights * s\r\n",
    "\r\n",
    "        # demodulation\r\n",
    "        if self.demodulate:\r\n",
    "            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\r\n",
    "            weights = weights * sigma_inv\r\n",
    "\r\n",
    "        # reshape input tensor for faster computation\r\n",
    "        x = x.reshape(1, -1, h, w)\r\n",
    "\r\n",
    "        # reshape weights for convolution\r\n",
    "        _, _, *weight_shapes = weights.shape\r\n",
    "        weights = weights.reshape(b * self.out_features, *weight_shapes)\r\n",
    "\r\n",
    "        # grouped convolution computation (repeat same convolution groups time along first axis)\r\n",
    "        x = F.conv2d(x, weights, padding=self.padding, groups=b)\r\n",
    "\r\n",
    "        # reshape to [batch_size, out_features, height, width]\r\n",
    "        return x.reshape(-1, self.out_features, h, w)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"style_block.svg\">\n",
    "\n",
    "Then the style block"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# The most basic building block of the generator\r\n",
    "class StyleBlock(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, d_latent : int, in_features : int, out_features : int):\r\n",
    "        '''\r\n",
    "        d_latent : the dimension of the input vector from the latent space\r\n",
    "        in_features : the number of features in the input feature map\r\n",
    "        out_features : the number of features in the output feature map\r\n",
    "        '''\r\n",
    "\r\n",
    "        super().__init__()\r\n",
    "        \r\n",
    "        # get style vector from latent space vector\r\n",
    "        self.to_style = EqualizedLinear(d_latent, in_features, bias=1.0)\r\n",
    "        \r\n",
    "        # compute modulated and demodulated convolution\r\n",
    "        self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\r\n",
    "\r\n",
    "        self.scale_noise = nn.Parameter(torch.zeros(1))\r\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\r\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\r\n",
    "\r\n",
    "    def forward(self, x : torch.Tensor, w : torch.Tensor, noise : Optional[torch.Tensor]):\r\n",
    "        # get style vector\r\n",
    "        s = self.to_style(w)\r\n",
    "        \r\n",
    "        # compute convolution operation\r\n",
    "        x = self.conv(x, s)\r\n",
    "\r\n",
    "        if noise is not None:\r\n",
    "            x += self.scale_noise[None, :, None, None] * noise\r\n",
    "        return self.activation(x + self.bias[None, :, None, None])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"to_rgb.svg\">\n",
    "\n",
    "ToRGB layer for the generator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class ToRGB(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, d_latent : int, features : int):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # get style vector from latent space vector\r\n",
    "        self.to_style = EqualizedLinear(d_latent, features, bias=1.0)\r\n",
    "\r\n",
    "        # compute modulated convolution\r\n",
    "        self.conv = Conv2dWeightModulate(features, 3, kernel_size=1, demodulate=False)\r\n",
    "\r\n",
    "        self.bias = nn.Parameter(torch.zeros(3))\r\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\r\n",
    "    \r\n",
    "    def forward(self, x : torch.Tensor, w : torch.Tensor):\r\n",
    "        s = self.to_style(w)\r\n",
    "        x = self.conv(x, s)\r\n",
    "\r\n",
    "        return self.activation(x + self.bias[None, :, None, None])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"generator_block.svg\">\n",
    "\n",
    "Putting it all together here is the generator block"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Generator network that generate an image from a latent vector\r\n",
    "class GeneratorBlock(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, d_latent : int, in_features : int, out_features : int):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.style_block1 = StyleBlock(d_latent, in_features, out_features)\r\n",
    "        self.style_block2 = StyleBlock(d_latent, out_features, out_features)\r\n",
    "\r\n",
    "        self.to_rgb = ToRGB(d_latent, out_features)\r\n",
    "    \r\n",
    "    def forward(self, x : torch.Tensor, w : torch.Tensor, noise: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]):\r\n",
    "\r\n",
    "        x = self.style_block1(x, w, noise[0])\r\n",
    "        x = self.style_block2(x, w, noise[1])\r\n",
    "\r\n",
    "        rgb = self.to_rgb(x, w)\r\n",
    "\r\n",
    "        return x, rgb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"style_gan2.svg\">\n",
    "\n",
    "And finaly the mighty generator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class Generator(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, log_resolution : int, d_latent : int, n_features : int = 32, max_features : int = 512):\r\n",
    "        '''\r\n",
    "        log_resolution : is the log2 of image resolution (used to calculate features)\r\n",
    "        d_latent : the dimensionality of the latent vector\r\n",
    "        n_features : number of features in the final convolution layer\r\n",
    "        max_features : maximum number of features in any generator block\r\n",
    "        '''\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # Calculate the number of features for each block\r\n",
    "        # looks like [512, 512, 256, 128, 64, 32]\r\n",
    "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 2, -1, -1)]\r\n",
    "\r\n",
    "        self.n_blocks = len(features)\r\n",
    "\r\n",
    "        # trainable constant \r\n",
    "        self.initial_constant = nn.Parameter(torch.randn((1, features[0], 4, 4)))\r\n",
    "\r\n",
    "        # Initial style block\r\n",
    "        self.style_block = StyleBlock(d_latent, features[0], features[0])\r\n",
    "        self.to_rgb = ToRGB(d_latent, features[0])\r\n",
    "\r\n",
    "        # generator blocks\r\n",
    "        blocks = [GeneratorBlock(d_latent, features[i - 1], features[i]) for i in range(1, self.n_blocks)]\r\n",
    "        self.blocks = nn.ModuleList(blocks)\r\n",
    "\r\n",
    "        # upsampling at each block\r\n",
    "        self.up_sample = UpSample()\r\n",
    "\r\n",
    "    def forward(self, w : torch.Tensor, input_noise : List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]]):\r\n",
    "        '''\r\n",
    "        w : In order to mix-styles (use different w for different layers), we provide a separate w for each generator block. It has shape [n_blocks, batch_size, d_latent]\r\n",
    "        input_noise : tuple of noise for each block\r\n",
    "        '''\r\n",
    "\r\n",
    "        batch_size = w.shape[1]\r\n",
    "\r\n",
    "        x = self.initial_constant.expand(batch_size, -1, -1, -1)\r\n",
    "        x = self.style_block(x, w[0], input_noise[0][1])\r\n",
    "        rgb = self.to_rgb(x, w[0])\r\n",
    "\r\n",
    "        for i in range(1, self.n_blocks):\r\n",
    "            x = self.up_sample(x)\r\n",
    "\r\n",
    "            x, rgb_new = self.blocks[i - 1](x, w[i], input_noise[i])\r\n",
    "            \r\n",
    "            rgb = self.up_sample(rgb) + rgb_new\r\n",
    "\r\n",
    "        return rgb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"discriminator_block.svg\">\n",
    "\n",
    "The discriminator block"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class DiscrimanatorBlock(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, in_features : int, out_features : int):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        # The skip connection line\r\n",
    "        self.residual = nn.Sequential(\r\n",
    "            DownSample(), \r\n",
    "            EqualizedConv2d(in_features, out_features, kernel_size=1)\r\n",
    "        )\r\n",
    "\r\n",
    "        # The main line\r\n",
    "        self.block = nn.Sequential(\r\n",
    "            EqualizedConv2d(in_features, in_features, kernel_size=3, padding=1),\r\n",
    "            nn.LeakyReLU(0.2, True),\r\n",
    "            EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\r\n",
    "            nn.LeakyReLU(0.2, True)\r\n",
    "       )\r\n",
    "\r\n",
    "        self.downsample = DownSample()\r\n",
    "\r\n",
    "        # Scaling after the residual adding operation\r\n",
    "        self.scale = 1 / math.sqrt(2)\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, x : torch.Tensor):\r\n",
    "        res = self.residual(x)\r\n",
    "        x = self.block(x)\r\n",
    "        x = self.downsample(x)\r\n",
    "        \r\n",
    "        return (x + res) * self.scale"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"style_gan2_disc.svg\">\n",
    "\n",
    "And the last one, The discriminator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class Discriminator(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, log_resolution : int, n_features : int = 64, max_features : int = 512):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.from_rgb = nn.Sequential(\r\n",
    "            EqualizedConv2d(3, n_features, 1),\r\n",
    "            nn.LeakyReLU(0.2, True),\r\n",
    "        )\r\n",
    "\r\n",
    "\r\n",
    "        # Calculate the number of features for each block.\r\n",
    "        # Something like [64, 128, 256, 512, 512, 512]        \r\n",
    "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\r\n",
    "\r\n",
    "        n_blocks = len(features) - 1\r\n",
    "\r\n",
    "        # Discriminator blocks\r\n",
    "        blocks = [DiscrimanatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\r\n",
    "        self.blocks = nn.Sequential(*blocks)\r\n",
    "\r\n",
    "        # Adding variation to the data with minibatch standard deviation\r\n",
    "        self.std_dev = MiniBatchStdDev()\r\n",
    "\r\n",
    "        final_features = features[-1] + 1\r\n",
    "\r\n",
    "        # Last conv layer\r\n",
    "        self.conv = EqualizedConv2d(final_features, final_features, 3)\r\n",
    "\r\n",
    "        # final linear classification\r\n",
    "        self.classify = EqualizedLinear(2 * 2 * final_features, 1)\r\n",
    "\r\n",
    "    def forward(self, x : torch.Tensor):\r\n",
    "        x = x - 0.5\r\n",
    "        x = self.from_rgb(x)\r\n",
    "\r\n",
    "        x = self.blocks(x)\r\n",
    "        x = self.std_dev(x)\r\n",
    "        x = self.conv(x)\r\n",
    "\r\n",
    "        x = x.reshape(x.shape[0], -1)\r\n",
    "        return self.classify(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the r1 regularization from the paper [Which Training Methods for GANs do actually Converge?](https://arxiv.org/abs/1801.04406).\n",
    "\n",
    "$ R_1 (\\psi) = \\frac{\\gamma}{2} \\mathbb{E}_{P_{D(x)}} \\left [  \\left \\| \\nabla_x D_{\\psi}(x)^2 \\right \\|\\right ] $\n",
    "\n",
    "it penalyze the discriminator for deviation from the [nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) by reducing the convergence speed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class GradientPenalty(nn.Module):\r\n",
    "    def forward(self, x : torch.Tensor, d : torch.Tensor):\r\n",
    "        gradients, *_ = torch.autograd.grad(outputs=d,\r\n",
    "                                            inputs=x,\r\n",
    "                                            grad_outputs=d.new_ones(d.shape),\r\n",
    "                                            retain_graph=True)\r\n",
    "\r\n",
    "        gradients = gradients.reshape(x.shape[0], -1)\r\n",
    "        norm = gradients.norm(2, dim=-1)\r\n",
    "\r\n",
    "        return torch.mean(norm ** 2) * (self.gamma / 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This regularization encourages a fixed-size step in w to result in a fixed-magnitude change in the image.\n",
    "\n",
    "we use this formula for regularization :\n",
    "\n",
    "$ E_{w∼f(z),y∼N(0,I)}(||J^⊤_wy||_2−a)^2 $\n",
    "\n",
    "to compute the jacobian, we will use :\n",
    "\n",
    "$ J_w^Ty = \\nabla_w(g(w) * y) $\n",
    "\n",
    "\n",
    "a is the exponential moving average of $ ||J^⊤_wy||^2 $ as the training progresses:\n",
    "\n",
    "$ a = \\frac{1}{1 - \\beta^N} \\sum_{i=1}^{N} \\beta^{(N - i)}[J_w^Ty]_i$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class PathLengthPenalty(nn.Module):\r\n",
    "    def __init__(self, beta: float):\r\n",
    "        super().__init__()\r\n",
    "        self.beta = beta\r\n",
    "\r\n",
    "        # Number of steps N\r\n",
    "        self.steps = nn.Parameter(torch.tensor(0.), requires_grad=False)\r\n",
    "        # Exponential sum of the jacobian time y\r\n",
    "        self.exp_sum_a = nn.Parameter(torch.tensor(0.), requires_grad=False)\r\n",
    "\r\n",
    "    def forward(self, w : torch.Tensor, x : torch.Tensor):\r\n",
    "        '''\r\n",
    "        w : the batch of w of shape [batch_size, d_latent]\r\n",
    "        x : the generated images of shape [batch_size, 3, height, width]\r\n",
    "        '''\r\n",
    "\r\n",
    "        device = x.device\r\n",
    "        image_size = x.shape[2] * x.shape[3]\r\n",
    "\r\n",
    "        # Calculate y ∈ N(0,I)\r\n",
    "        y = torch.randn(x.shape, device=device)\r\n",
    "\r\n",
    "        # Calculate (g(w)⋅y) and normalize by the square root of image size. \r\n",
    "        # We can find it in their implementation.\r\n",
    "        output = (x * y).sum() / math.sqrt(image_size)\r\n",
    "\r\n",
    "        gradients, *_ = torch.autograd.grad(output=output,\r\n",
    "                                            inputs=w,\r\n",
    "                                            grad_outputs=torch.ones(output.shape, device=device),\r\n",
    "                                            create_graph=True)\r\n",
    "\r\n",
    "        norm = (gradients ** 2).sum(dim=2).mean(dim=1).sqrt()\r\n",
    "\r\n",
    "        if self.steps > 0:\r\n",
    "            # let's calculate a with the exponential sum of moving average\r\n",
    "            a = self.exp_sum_a / (1 - self.beta ** self.steps)\r\n",
    "            # and finaly the penalty\r\n",
    "            loss = torch.mean((norm - a) ** 2)\r\n",
    "        else:\r\n",
    "            # do not penalyze the first step because we can't have a moving average\r\n",
    "            loss = norm.new_tensor(0)\r\n",
    "        \r\n",
    "        mean = norm.mean().detach()\r\n",
    "        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\r\n",
    "        self.steps.add_(1.)\r\n",
    "        \r\n",
    "        return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class Dataset(data.Dataset):\r\n",
    "    def __init__(self, path: str, image_size: int):\r\n",
    "        super().__init__()\r\n",
    "        self.paths = [p for p in Path(path).glob(f'**\\\\*.jpg')] + [p for p in Path(path).glob(f'**\\\\*.png')]\r\n",
    "        self.transform = torchvision.transforms.Compose([\r\n",
    "            torchvision.transforms.Resize(image_size),\r\n",
    "            torchvision.transforms.RandomCrop(image_size),\r\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\r\n",
    "            torchvision.transforms.ColorJitter(.15,.2,.2),\r\n",
    "            torchvision.transforms.ToTensor(),\r\n",
    "            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True),\r\n",
    "        ])\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.paths)\r\n",
    "    \r\n",
    "    def __getitem__(self, index):\r\n",
    "        path = self.paths[index]\r\n",
    "        img = Image.open(path).convert(\"RGB\")\r\n",
    "        return self.transform(img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# _dataset = Dataset(\"E:\\\\projects\\\\StyleGan2Pytorch\\\\dataset\", 1024)\r\n",
    "\r\n",
    "# for i in range(10):\r\n",
    "#     batch = torchvision.transforms.ToPILImage()(_dataset[14242])\r\n",
    "#     plt.imshow(batch)\r\n",
    "#     plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " We want to find $w$ to maximize\n",
    "$ \\mathbb{E}_{x \\sim \\mathbb{P}_r} [f_w(x)]- \\mathbb{E}_{z \\sim p(z)} [f_w(g_\\theta(z))] $,\n",
    "so we minimize,\n",
    "$ -\\frac{1}{m} \\sum_{i=1}^m f_w \\big(x^{(i)} \\big) + \\frac{1}{m} \\sum_{i=1}^m f_w \\big( g_\\theta(z^{(i)}) \\big) $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class DiscriminatorLoss(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    A big score will represent a real image. So we want the fake_score to be small and the real_score\r\n",
    "    to be high. So we penalyze the discriminator the other way around. We use softplus because it helps\r\n",
    "    the the discriminator learn even if the score is a small negative. \r\n",
    "    \"\"\"\r\n",
    "    def __call__(self, real_score : torch.Tensor, fake_score : torch.Tensor):\r\n",
    "        return F.softplus(-real_score).mean(), F.softplus(fake_score).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " We want to find $\\theta$ to minimize\n",
    "    $$\\mathbb{E}_{x \\sim \\mathbb{P}_r} [f_w(x)]- \\mathbb{E}_{z \\sim p(z)} [f_w(g_\\theta(z))]$$\n",
    "    The first component is independent of $\\theta$,\n",
    "    so we minimize,\n",
    "    $$-\\frac{1}{m} \\sum_{i=1}^m f_w \\big( g_\\theta(z^{(i)}) \\big)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "class GeneratorLoss(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    The more the fake_score is big, the more the discriminator believe that the fake image is real\r\n",
    "    so we penalyze the generator when the discriminator return a small fake_score.  \r\n",
    "    \"\"\"\r\n",
    "    def __call__(self, fake_score: torch.Tensor):\r\n",
    "        return F.softplus(-fake_score).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def infinite_loader(dataloader):\r\n",
    "    while True:\r\n",
    "        for batch in dataloader:\r\n",
    "            yield batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def accumulate(model1, model2, decay=0.999):\r\n",
    "    assert decay < 1, \"decay must be smaller than one\"\r\n",
    "    params1 = dict(model1.named_parameters())\r\n",
    "    params2 = dict(model2.named_parameters())\r\n",
    "    \r\n",
    "    for k in params1.keys():\r\n",
    "        params1[k].data.mul_(decay).add_(params2[k].data, alpha=1 - decay)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import json\r\n",
    "\r\n",
    "class DevEnvironment():\r\n",
    "\r\n",
    "    def __init__(self, config_file):\r\n",
    "        '''\r\n",
    "        config_file: path to the config file\r\n",
    "        '''\r\n",
    "        config = json.loads(open(config_file).read())\r\n",
    "        self.optimizer = config[\"optimizer\"] if \"optimizer\" in config else \"Adam\"\r\n",
    "\r\n",
    "        self.use_gradient_penalty = config[\"use_gradient_penalty\"] if \"use_gradient_penalty\" in config else True\r\n",
    "        self.gradient_penalty_coefficient = config[\"gradient_penalty_coefficient\"] if \"gradient_penalty_coefficient\" in config else 10\r\n",
    "        self.lazy_gradient_penalty_interval = config[\"lazy_gradient_penalty_interval\"] if \"lazy_gradient_penalty_interval\" in config else 4\r\n",
    "        self.use_path_length_penalty = config[\"use_path_length_penalty\"] if \"use_path_length_penalty\" in config else True\r\n",
    "        self.lazy_path_penalty_interval = config[\"lazy_path_penalty_interval\"] if \"lazy_path_penalty_interval\" in config else 32\r\n",
    "        self.lazy_path_penalty_after = config[\"lazy_path_penalty_after\"] if \"lazy_path_penalty_after\" in config else 5000\r\n",
    "        self.batch_size = config[\"batch_size\"] if \"batch_size\" in config else 16\r\n",
    "        self.d_latent = config[\"d_latent\"] if \"d_latent\" in config else 512\r\n",
    "        self.max_features = config[\"max_features\"] if \"max_features\" in config else 1024\r\n",
    "        self.image_size = config[\"image_size\"] if \"image_size\" in config else 1024\r\n",
    "        \r\n",
    "        self.mapping_network_layers = config[\"mapping_network_layers\"] if \"mapping_network_layers\" in config else 8\r\n",
    "        \r\n",
    "        self.learning_rate_discr = config[\"learning_rate_discr\"] if \"learning_rate_discr\" in config else 4e-4\r\n",
    "        self.learning_rate_gen = config[\"learning_rate_gen\"] if \"learning_rate_gen\" in config else 1e-4\r\n",
    "        self.mapping_network_learning_rate = config[\"mapping_network_learning_rate\"] if \"mapping_network_learning_rate\" in config else 1e-5\r\n",
    "        \r\n",
    "        self.gradient_accumulate_steps = config[\"gradient_accumulate_steps\"] if \"gradient_accumulate_steps\" in config else 1\r\n",
    "        self.style_mixing_prob = config[\"style_mixing_prob\"] if \"style_mixing_prob\" in config else 0.9\r\n",
    "        self.training_steps = config[\"training_steps\"] if \"training_steps\" in config else 500_000\r\n",
    "        \r\n",
    "        self.save_checkpoint_interval = config[\"save_checkpoint_interval\"] if \"save_checkpoint_interval\" in config else 2000\r\n",
    "        self.log_generated_interval = config[\"log_generated_interval\"] if \"log_generated_interval\" in config else 500\r\n",
    "        \r\n",
    "        self.reset_EMA = config[\"reset_EMA\"] if \"reset_EMA\" in config else 5000\r\n",
    "        self.accumulate = config[\"accumulate\"] if \"accumulate\" in config else 0.997\r\n",
    "        \r\n",
    "        self.dataset_path = config[\"dataset_path\"] if \"dataset_path\" in config else \"E:\\\\projects\\\\StyleGan2Pytorch\\\\gallery-dl\\\\flickr\\\\Groups\\\\Landscape\"\r\n",
    "        \r\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n",
    "        \r\n",
    "        self.log_resolution = int(np.log2(self.image_size))\r\n",
    "\r\n",
    "        self.mapping_network = MappingNetwork(self.d_latent, self.mapping_network_layers).to(self.device)\r\n",
    "        self.discriminator = Discriminator(self.log_resolution).to(self.device)\r\n",
    "        self.generator = Generator(self.log_resolution, self.d_latent).to(self.device)\r\n",
    "        self.generator_ema = Generator(self.log_resolution, self.d_latent).to(self.device)\r\n",
    "\r\n",
    "        self.generator_ema.eval()\r\n",
    "\r\n",
    "        accumulate(self.generator_ema, self.generator, 0)\r\n",
    "\r\n",
    "        self.n_gen_blocks = self.generator.n_blocks\r\n",
    "        \r\n",
    "        dataset = Dataset(self.dataset_path, self.image_size)\r\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size = self.batch_size, shuffle=True, drop_last=True, pin_memory=True)\r\n",
    "        self.loader = infinite_loader(dataloader)\r\n",
    "\r\n",
    "        self.path_length_penalty = PathLengthPenalty(0.99).to(self.device)\r\n",
    "        self.gradient_penalty = GradientPenalty()\r\n",
    "\r\n",
    "        self.discriminator_loss = DiscriminatorLoss().to(self.device)\r\n",
    "        self.generator_loss = GeneratorLoss().to(self.device)\r\n",
    "\r\n",
    "        self.adam_betas = (0.0, 0.99)\r\n",
    "\r\n",
    "        self.discriminator_optimizer = torch.optim.Adam(\r\n",
    "            self.discriminator.parameters(),\r\n",
    "            lr=self.learning_rate_discr, betas=self.adam_betas\r\n",
    "        )\r\n",
    "        self.generator_optimizer = torch.optim.Adam(\r\n",
    "            self.generator.parameters(),\r\n",
    "            lr=self.learning_rate_gen, betas=self.adam_betas\r\n",
    "        )\r\n",
    "        self.mapping_network_optimizer = torch.optim.Adam(\r\n",
    "            self.mapping_network.parameters(),\r\n",
    "            lr=self.mapping_network_learning_rate, betas=self.adam_betas\r\n",
    "        )\r\n",
    "    \r\n",
    "    ''' to load\r\n",
    "    \"mnet\", \"g\", \"d\", \"mnet_optim\", \"g_optim\", \"d_optim\"\r\n",
    "    '''\r\n",
    "    def laod_save(self, file_path : str):\r\n",
    "        saved = torch.load(file_path, map_location=lambda storage, loc: storage)\r\n",
    "\r\n",
    "        self.generator.load_state_dict(saved[\"g\"], strict=False)\r\n",
    "        self.mapping_network.load_state_dict(saved[\"mnet\"], strict=False)\r\n",
    "        self.discriminator.load_state_dict(saved[\"d\"], strict=False)\r\n",
    "        self.mapping_network_optimizer.load_state_dict(saved[\"mnet_optim\"])\r\n",
    "        self.generator_optimizer.load_state_dict(saved[\"g_optim\"])\r\n",
    "        self.discriminator_optimizer.load_state_dict(saved[\"d_optim\"])\r\n",
    "        accumulate(self.generator_ema, self.generator, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "env = DevEnvironment(\"config.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "sample a random vector $ z $ and get $ w $ from the mapping network. Also do style mixing with probability style_mixing_prob"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def sample_w(env : DevEnvironment):\r\n",
    "    if np.random.rand() < env.style_mixing_prob:\r\n",
    "        cross_over_point = int(np.random.rand() * env.n_gen_blocks)\r\n",
    "\r\n",
    "        z1 = torch.randn(env.batch_size, env.d_latent).to(env.device)\r\n",
    "        z2 = torch.randn(env.batch_size, env.d_latent).to(env.device)\r\n",
    "\r\n",
    "        w1 = env.mapping_network(z1)[None, :, :].expand(cross_over_point, -1, -1)\r\n",
    "        w2 = env.mapping_network(z2)[None, :, :].expand(env.n_gen_blocks - cross_over_point, -1, -1)\r\n",
    "\r\n",
    "        return torch.cat((w1, w2), dim=0)\r\n",
    "\r\n",
    "    else:\r\n",
    "        z = torch.randn(env.batch_size, env.d_latent).to(env.device)\r\n",
    "\r\n",
    "        w = env.mapping_network(z)\r\n",
    "    \r\n",
    "        return w[None, :, :].expand(env.n_gen_blocks, -1, -1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def sample_noise(env : DevEnvironment):\r\n",
    "    noise = []\r\n",
    "    resolution = 4\r\n",
    "    for i in range(env.n_gen_blocks):\r\n",
    "        # First layer only have one style block\r\n",
    "        if i == 0:\r\n",
    "            n1 = None\r\n",
    "        else:\r\n",
    "            n1 = torch.randn(env.batch_size, 1, resolution, resolution, device=env.device)\r\n",
    "        n2 = torch.randn(env.batch_size, 1, resolution, resolution, device=env.device)\r\n",
    "        resolution *= 2\r\n",
    "        noise.append((n1, n2))\r\n",
    "    return noise;"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def generate_images(env: DevEnvironment):\r\n",
    "    w = sample_w(env)\r\n",
    "\r\n",
    "    noise = sample_noise(env)\r\n",
    "\r\n",
    "    images = env.generator(w, noise)\r\n",
    "    return images, w"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def requires_grad(model, flag=True):\r\n",
    "    for p in model.parameters():\r\n",
    "        p.requires_grad = flag"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import gc\r\n",
    "def clean():\r\n",
    "    torch.cuda.empty_cache()\r\n",
    "    gc.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def step(index : int, env: DevEnvironment):\r\n",
    "    with torch.cuda.amp.autocast():\r\n",
    "        real_img = next(env.loader).to(env.device)\r\n",
    "    requires_grad(env.generator, False)\r\n",
    "    requires_grad(env.mapping_network, False)\r\n",
    "    requires_grad(env.discriminator, True)\r\n",
    "    \r\n",
    "    with torch.cuda.amp.autocast():\r\n",
    "        fake_img, _ = generate_images(env)\r\n",
    "        fake_pred = env.discriminator(fake_img)\r\n",
    "\r\n",
    "        real_pred = env.discriminator(real_img)\r\n",
    "        real_loss, fake_loss = env.discriminator_loss(real_pred, fake_pred)\r\n",
    "        \r\n",
    "        d_loss = real_loss + fake_loss\r\n",
    "    \r\n",
    "    env.discriminator.zero_grad()\r\n",
    "    d_loss.backward()\r\n",
    "\r\n",
    "    torch.nn.utils.clip_grad_norm_(env.discriminator.parameters(), max_norm=1.0)\r\n",
    "    torch.nn.utils.clip_grad_value_(env.discriminator.parameters(), 1.0)\r\n",
    "    for params in env.discriminator.parameters():\r\n",
    "        torch.nan_to_num_(params.grad, nan=0, posinf=1e5, neginf=-1e5)\r\n",
    "\r\n",
    "    env.discriminator_optimizer.step()\r\n",
    "\r\n",
    "    if (index + 1) % env.lazy_gradient_penalty_interval == 0:\r\n",
    "        with torch.cuda.amp.autocast():\r\n",
    "            real_img.requires_grad_()\r\n",
    "            real_pred = env.discriminator(real_img)\r\n",
    "            gp = env.gradient_penalty(real_img, real_pred) * env.gradient_penalty_coefficient\r\n",
    "        env.discriminator.zero_grad()\r\n",
    "        gp.backward()\r\n",
    "\r\n",
    "        torch.nn.utils.clip_grad_norm_(env.discriminator.parameters(), max_norm=1.0)\r\n",
    "        torch.nn.utils.clip_grad_value_(env.discriminator.parameters(), 1.0)\r\n",
    "        for params in env.discriminator.parameters():\r\n",
    "            torch.nan_to_num_(params.grad, nan=0, posinf=1e5, neginf=-1e5)\r\n",
    "        env.discriminator_optimizer.step()\r\n",
    "\r\n",
    "    requires_grad(env.generator, True)\r\n",
    "    requires_grad(env.mapping_network, True)\r\n",
    "    requires_grad(env.discriminator, False)\r\n",
    "\r\n",
    "    with torch.cuda.amp.autocast():\r\n",
    "        fake_img, _ = generate_images(env)\r\n",
    "        fake_pred = env.discriminator(fake_img)\r\n",
    "        \r\n",
    "        g_loss = env.generator_loss(fake_pred)\r\n",
    "\r\n",
    "    env.generator.zero_grad()\r\n",
    "    env.mapping_network.zero_grad()\r\n",
    "    g_loss.backward()\r\n",
    "\r\n",
    "    torch.nn.utils.clip_grad_norm_(env.generator.parameters(), max_norm=1.0)\r\n",
    "    torch.nn.utils.clip_grad_value_(env.generator.parameters(), 1.0)\r\n",
    "    torch.nn.utils.clip_grad_norm_(env.mapping_network.parameters(), max_norm=1.0)\r\n",
    "    torch.nn.utils.clip_grad_value_(env.mapping_network.parameters(), 1.0)\r\n",
    "    for params in env.generator.parameters():\r\n",
    "        torch.nan_to_num_(params.grad, nan=0, posinf=1e5, neginf=-1e5)\r\n",
    "    for params in env.mapping_network.parameters():\r\n",
    "        torch.nan_to_num_(params.grad, nan=0, posinf=1e5, neginf=-1e5)\r\n",
    "\r\n",
    "    env.generator_optimizer.step()\r\n",
    "    env.mapping_network_optimizer.step()\r\n",
    "\r\n",
    "    if index > env.lazy_path_penalty_after and (index + 1) % env.lazy_path_penalty_interval == 0:      \r\n",
    "        with torch.cuda.amp.autocast():\r\n",
    "            fake_img, w = generate_images(env)\r\n",
    "            \r\n",
    "            plp = env.path_length_penalty(w, fake_img)\r\n",
    "            plp = 2 * env.lazy_path_penalty_interval * plp\r\n",
    "        \r\n",
    "        env.generator.zero_grad()\r\n",
    "        plp.backward()\r\n",
    "\r\n",
    "        torch.nn.utils.clip_grad_norm_(env.generator.parameters(), max_norm=1.0)\r\n",
    "        torch.nn.utils.clip_grad_value_(env.generator.parameters(), 1.0)\r\n",
    "        torch.nn.utils.clip_grad_norm_(env.mapping_network.parameters(), max_norm=1.0)\r\n",
    "        torch.nn.utils.clip_grad_value_(env.mapping_network.parameters(), 1.0)\r\n",
    "        for params in env.generator.parameters():\r\n",
    "            torch.nan_to_num_(params.grad, nan=0, posinf=1e5, neginf=-1e5)\r\n",
    "        for params in env.mapping_network.parameters():\r\n",
    "            torch.nan_to_num_(params.grad, nan=0, posinf=1e5, neginf=-1e5)\r\n",
    "\r\n",
    "        env.generator_optimizer.step()\r\n",
    "\r\n",
    "    if not (index + 1) % env.reset_EMA:\r\n",
    "        accumulate(env.generator_ema, env.generator, 0)\r\n",
    "    else:\r\n",
    "        accumulate(env.generator_ema, env.generator, env.accumulate)\r\n",
    "\r\n",
    "    return d_loss.detach(), g_loss.detach()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# record all the losses to find eventual bugs\r\n",
    "loss_record = {\"gen\": [], \"discr\": []}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def plot_losses(loss_record):\r\n",
    "    clear_output(wait=True)\r\n",
    "    plt.plot(range(len(loss_record[\"gen\"])), loss_record[\"gen\"], 'g', label='generator loss')\r\n",
    "    plt.plot(range(len(loss_record[\"discr\"])), loss_record[\"discr\"], 'b', label='discriminator loss')\r\n",
    "    plt.title('Training and Validation accuracy')\r\n",
    "    plt.xlabel('Steps')\r\n",
    "    plt.ylabel('Loss')\r\n",
    "    plt.legend()\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll do the next cell just one time and then we will just load this from a file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# generate the latents used for sampling\r\n",
    "\r\n",
    "N_SAMP = 16\r\n",
    "'''\r\n",
    "example_z = torch.randn(N_SAMP, env.d_latent).to(env.device)\r\n",
    "example_w = env.mapping_network(example_z).detach()\r\n",
    "example_w = example_w[None, :, :].expand(env.n_gen_blocks, -1, -1)\r\n",
    "example_noise = []\r\n",
    "resolution = 4\r\n",
    "for i in range(env.n_gen_blocks):\r\n",
    "    # First layer only have one style block\r\n",
    "    if i == 0:\r\n",
    "        n1 = None\r\n",
    "    else:\r\n",
    "        n1 = torch.randn(N_SAMP, 1, resolution, resolution, device=env.device)\r\n",
    "    n2 = torch.randn(N_SAMP, 1, resolution, resolution, device=env.device)\r\n",
    "    resolution *= 2\r\n",
    "    example_noise.append((n1, n2))\r\n",
    "del example_z\r\n",
    "torch.save({\"sample_w\": example_w, \"sample_noise\": example_noise}, \"sample_tensors.pt\")\r\n",
    "del example_w\r\n",
    "del example_noise\r\n",
    "clean()\r\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nexample_z = torch.randn(N_SAMP, env.d_latent).to(env.device)\\nexample_w = env.mapping_network(example_z).detach()\\nexample_w = example_w[None, :, :].expand(env.n_gen_blocks, -1, -1)\\nexample_noise = []\\nresolution = 4\\nfor i in range(env.n_gen_blocks):\\n    # First layer only have one style block\\n    if i == 0:\\n        n1 = None\\n    else:\\n        n1 = torch.randn(N_SAMP, 1, resolution, resolution, device=env.device)\\n    n2 = torch.randn(N_SAMP, 1, resolution, resolution, device=env.device)\\n    resolution *= 2\\n    example_noise.append((n1, n2))\\ndel example_z\\ntorch.save({\"sample_w\": example_w, \"sample_noise\": example_noise}, \"sample_tensors.pt\")\\ndel example_w\\ndel example_noise\\nclean()\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "#Load the sample tensors\r\n",
    "saved = torch.load( \"sample_tensors.pt\", map_location=lambda storage, loc: storage)\r\n",
    "sampled_w = saved[\"sample_w\"].to(env.device)\r\n",
    "sampled_noise = [(a.to(env.device), b.to(env.device)) if i != 0 else (a, b.to(env.device)) for i, (a, b) in enumerate(saved[\"sample_noise\"])]\r\n",
    "\r\n",
    "#Hard coded\r\n",
    "start_iteration=27800\r\n",
    "\r\n",
    "pbar = tqdm(range(start_iteration, env.training_steps))\r\n",
    "\r\n",
    "if (start_iteration != 0):\r\n",
    "    env.laod_save(f\"checkpoint/{str(start_iteration).zfill(6)}.pt\")\r\n",
    "\r\n",
    "for i in pbar:\r\n",
    "    clean()\r\n",
    "    if not ((i + 1) % env.log_generated_interval):\r\n",
    "        with torch.no_grad():\r\n",
    "            env.generator_ema.eval()\r\n",
    "            images = env.generator_ema(sampled_w, sampled_noise)\r\n",
    "            torchvision.utils.save_image(\r\n",
    "                images,\r\n",
    "                f\"sample/{str(i+1).zfill(6)}.png\",\r\n",
    "                nrow=int(4),\r\n",
    "                normalize=True,\r\n",
    "                value_range=(-1, 1),\r\n",
    "            )\r\n",
    "            del images\r\n",
    "        clean()\r\n",
    "        plot_losses(loss_record)\r\n",
    "    if not ((i + 1) % env.save_checkpoint_interval):\r\n",
    "        torch.save(\r\n",
    "            {\r\n",
    "                \"mnet\": env.mapping_network.state_dict(),\r\n",
    "                \"g\": env.generator.state_dict(),\r\n",
    "                \"d\": env.discriminator.state_dict(),\r\n",
    "                \"mnet_optim\": env.mapping_network_optimizer.state_dict(),\r\n",
    "                \"g_optim\": env.generator_optimizer.state_dict(),\r\n",
    "                \"d_optim\": env.discriminator_optimizer.state_dict(),\r\n",
    "            },\r\n",
    "            f\"checkpoint/{str(i+1).zfill(6)}.pt\",\r\n",
    "        )\r\n",
    "    discriminator_loss, generator_loss = step(0, env)\r\n",
    "    if pbar.n == env.training_steps:\r\n",
    "        break\r\n",
    "    loss_record[\"gen\"].append(generator_loss.cpu())\r\n",
    "    loss_record[\"discr\"].append(discriminator_loss.cpu())\r\n",
    "    pbar.set_description(\"disc_loss: {}, gen_loss: {}\".format(discriminator_loss, generator_loss))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "disc_loss: 0.12104485929012299, gen_loss: 9.014169692993164:   1%|          | 2416/474600 [1:44:07<709:09:09,  5.41s/it]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "plot_losses(loss_record)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plot_losses' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-45f251c73d79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_record\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_losses' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}